{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning in scikit-learn\n",
    "\n",
    "This notebook shows how to use a number of different types of [ensembles](https://scikit-learn.org/stable/modules/ensemble.html) in scikit-learn. We use the Adult dataset to exemplify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data and preprocessing\n",
    "\n",
    "What we do here is probably going to be a bit more obvious after the next lecture, where we discuss preprocessing.\n",
    "\n",
    "We use the same dataset as we'll use elsewhere in the course (among other places, in Programming assignment 2). The task here is a binary classification task, where we want to predict whether someone earns more than 50K dollars a year or not, given a set of demographic features. The dataset comes with a pre-defined train/test split and you can download the [training set](http://www.cse.chalmers.se/~richajo/dit866/data/adult_train.csv) and the [test set](http://www.cse.chalmers.se/~richajo/dit866/data/adult_test.csv) as separate files.\n",
    "\n",
    "As in PA 2, we convert the rows of the Pandas dataframe into dictionaries, which works nicely with the `DictVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('adult_train.csv')\n",
    "\n",
    "n_cols = len(train_data.columns)\n",
    "Xtrain_dicts = train_data.iloc[:, :n_cols-1].to_dict('records')\n",
    "Ytrain = train_data.iloc[:, n_cols-1]\n",
    "\n",
    "test_data = pd.read_csv('adult_test.csv')\n",
    "Xtest_dicts = test_data.iloc[:, :n_cols-1].to_dict('records')\n",
    "Ytest = test_data.iloc[:, n_cols-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give you a feel for the dataset, here are the first five rows. We want to predict the `target` column, given the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46</td>\n",
       "      <td>Private</td>\n",
       "      <td>5th-6th</td>\n",
       "      <td>3</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Amer-Indian-Eskimo</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>1902</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>2824</td>\n",
       "      <td>76</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass     education  education-num      marital-status  \\\n",
       "0   27   Private  Some-college             10            Divorced   \n",
       "1   27   Private     Bachelors             13       Never-married   \n",
       "2   25   Private    Assoc-acdm             12  Married-civ-spouse   \n",
       "3   46   Private       5th-6th              3  Married-civ-spouse   \n",
       "4   45   Private          11th              7            Divorced   \n",
       "\n",
       "         occupation   relationship                race     sex  capital-gain  \\\n",
       "0      Adm-clerical      Unmarried               White  Female             0   \n",
       "1    Prof-specialty  Not-in-family               White  Female             0   \n",
       "2             Sales        Husband               White    Male             0   \n",
       "3  Transport-moving        Husband  Amer-Indian-Eskimo    Male             0   \n",
       "4  Transport-moving  Not-in-family               White    Male             0   \n",
       "\n",
       "   capital-loss  hours-per-week native-country target  \n",
       "0             0              44  United-States  <=50K  \n",
       "1             0              40  United-States  <=50K  \n",
       "2             0              40  United-States  <=50K  \n",
       "3          1902              40  United-States  <=50K  \n",
       "4          2824              76  United-States   >50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the representation of the first individual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 27,\n",
       " 'workclass': 'Private',\n",
       " 'education': 'Some-college',\n",
       " 'education-num': 10,\n",
       " 'marital-status': 'Divorced',\n",
       " 'occupation': 'Adm-clerical',\n",
       " 'relationship': 'Unmarried',\n",
       " 'race': 'White',\n",
       " 'sex': 'Female',\n",
       " 'capital-gain': 0,\n",
       " 'capital-loss': 0,\n",
       " 'hours-per-week': 44,\n",
       " 'native-country': 'United-States'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_dicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with scikit-learn, we need to convert the symbolic features into a numerical matrix. Here is how we do this. Again, this is going to be clearer after the next lecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 13 stored elements and shape (1, 107)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic preprocessing stuff\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "preprocessing_pipeline = make_pipeline(DictVectorizer(), StandardScaler(with_mean=False))\n",
    "\n",
    "Xtrain = preprocessing_pipeline.fit_transform(Xtrain_dicts)\n",
    "Xtest = preprocessing_pipeline.transform(Xtest_dicts)\n",
    "\n",
    "Xtrain[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Building an ensemble of any set of classifiers\n",
    "\n",
    "The following example shows how to combine a set of classifiers into an ensemble. This can be done simply in scikit-learn by using a [`VotingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html). Here, we combine two types of logistic regression, a decision tree, and a neural network.\n",
    "\n",
    "By default, the `VotingClassifier` will use voting to compute the final prediction. By using the option `voting='soft'`, the `VotingClassifier` will use averaging of probabilities instead. Note that this requires a probability-aware classifier: it needs to have a method called `predict_proba`.\n",
    "\n",
    "The option `n_jobs=-1` is for efficiency and simply means that we use all available processors on the machine and run the training of the submodels in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# a few different types of classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# turn off annoying warnings\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# and the VotingClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8551071801486395"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble = [\n",
    "            ('lr', LogisticRegression()),\n",
    "            ('dt', DecisionTreeClassifier(max_depth=5)),\n",
    "            ('lr1', LogisticRegression(penalty='l1', solver='liblinear')),\n",
    "            ('mlp', MLPClassifier(hidden_layer_sizes=(8), max_iter=10000))\n",
    "           ]\n",
    "\n",
    "voting = VotingClassifier(ensemble)\n",
    "#voting = VotingClassifier(ensemble, voting='soft')\n",
    "\n",
    "voting.fit(Xtrain, Ytrain)\n",
    "\n",
    "accuracy_score(Ytest, voting.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "We can create an ensemble using stacking in more or less the same way. This will take a bit of time, because cross-validation is used during training. (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8557213930348259"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "ensemble = [\n",
    "            ('lr', LogisticRegression()),\n",
    "            ('dt', DecisionTreeClassifier(max_depth=5)),\n",
    "            ('lr1', LogisticRegression(penalty='l1', solver='liblinear')),\n",
    "            ('mlp', MLPClassifier(hidden_layer_sizes=(8), max_iter=10000))\n",
    "           ]\n",
    "\n",
    "stacking = StackingClassifier(ensemble)\n",
    "\n",
    "stacking.fit(Xtrain, Ytrain)\n",
    "\n",
    "accuracy_score(Ytest, stacking.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an ensemble using bagging and random subspace learning\n",
    "\n",
    "In contrast to the example above, where we just combined a few different classifiers, we will now see how an ensemble can be created in a way where we more systematically try to achieve a diversity among the classifiers. We will use decision trees in this example.\n",
    "\n",
    "Before we do that, let's see what kind of accuracy we get when we use a single decision tree with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.818500092131933"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "tree.fit(Xtrain, Ytrain)\n",
    "accuracy_score(Ytest, tree.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) creates an ensemble using the [*bagging*](https://en.wikipedia.org/wiki/Bootstrap_aggregating) method and/or [*random subspace learning*](https://en.wikipedia.org/wiki/Random_subspace_method) (\"feature bagging\"). In bagging, diversity of sub-classifiers is achieved by selecting new training sets from the original set by drawing *instances* with replacement. In random subspace learning, the different sub-classifiers instead use different subsets of *features*.\n",
    "\n",
    "By setting `bootstrap=True` (this is true by default), bagging is enabled, and random subspace learning is turned on by setting `bootstrap_feature=True`. As you can see, by turning on both options, we can get an accuracy in the 0.85-0.86 range by turning on these features when using an ensemble. The exact accuracy you get will depend on how random sampling of instances and features is done; you can get reproducible results by setting the `random_state`. In general, you will get a higher accuracy when using a larger number of sub-classifiers (`n_estimators`) and there is no risk of overfitting by increasing this value, but this will of course make the ensemble slower.\n",
    "\n",
    "Random Subspace Learning (also called feature bagging) is a technique where each base learner in an ensemble is **trained on a random subset of features** rather than using all features. This helps introduce more diversity among the base learners, improving generalization and reducing overfitting. Helps reduce variance by averaging multiple diverse models trained on different data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance bootstrapping: False; feature bootstrapping: False; accuracy: 0.819\n",
      "Instance bootstrapping: False; feature bootstrapping: True; accuracy: 0.847\n",
      "Instance bootstrapping: True; feature bootstrapping: False; accuracy: 0.844\n",
      "Instance bootstrapping: True; feature bootstrapping: True; accuracy: 0.851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "for bootstrap_instances in [False, True]:\n",
    "    for bootstrap_features in [False, True]:\n",
    "        bagging = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                                    n_estimators=10, \n",
    "                                    bootstrap=bootstrap_instances, bootstrap_features=bootstrap_features, \n",
    "                                    random_state=0, n_jobs=-1)\n",
    "        \n",
    "\n",
    "        bagging.fit(Xtrain, Ytrain)\n",
    "\n",
    "        acc = accuracy_score(Ytest, bagging.predict(Xtest))\n",
    "\n",
    "        print(f'Instance bootstrapping: {bootstrap_instances}; feature bootstrapping: {bootstrap_features}; accuracy: {acc:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests\n",
    "\n",
    "The [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) uses the [*random forest*](https://en.wikipedia.org/wiki/Random_forest) method to build ensembles of decision trees. This ensemble training method uses training set bagging as well as random subspace learning each time a feature is selected when building the decision trees. This is usually a high-quality model for \"tabular\" data: that is, a set of named columns, such as what we get if we load a CSV or Excel file using Pandas. This is also the situation we have here.\n",
    "\n",
    "As in the `BaggingClassifier`, the main hyperparameter to adjust when constructing the ensemble is the number of sub-trees used in the ensemble (`n_estimators`). Apart from that, the `RandomForestClassifier` (and the equivalent model for regression, `RandomForestRegression`) has a number of hyperparameter controlling the tree building, similar to a `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8624777347828757"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=0, n_jobs=-1)\n",
    "\n",
    "rf.fit(Xtrain, Ytrain)\n",
    "accuracy_score(Ytest, rf.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important hyperparameters for random forests:\n",
    "\n",
    "- `n_estimators` controls the size of the ensemble\n",
    "- `max_features`: how many features to consider when splitting; by default, sqrt(n_features)\n",
    "- tree-related hyperparameters including `max_depth`\n",
    "- `n_jobs` for how many CPU cores to use\n",
    "- `random_state` for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
