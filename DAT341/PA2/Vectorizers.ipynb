{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizers for converting features into numerical vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following small examples show how the various vectorizers in scikit-learn work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DictVectorizer, for encoding features stored in dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make a small training set where the instances consist off a mix of symbolic, Boolean, and numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [{'f1':'B', 'f2':'F', 'f3':False, 'f4':7},\n",
    "    {'f1':'B', 'f2':'M', 'f3':True, 'f4':2},\n",
    "    {'f1':'O', 'f2':'F', 'f3':False, 'f4':9}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`DictVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) is used when features are stored in dictionaries.\n",
    "\n",
    "We call `fit_transform`, which is equivalent to first calling `fit` and then `transform`. `fit` goes through the training set and builds a vocabulary of features. `transform` can then carry out the conversion from the list of dictionaries into a numerical matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vec = DictVectorizer()\n",
    "Xe = vec.fit_transform(X)\n",
    "\n",
    "# We use toarray in order to convert from a sparse matrix\n",
    "# into a normal matrix, so that we can print the matrix nicely.\n",
    "Xe.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the vocabulary of features. This can be useful, for instance, when we need to interpret the weights of a linear classifier or regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer, for encoding \"bags of words\" (typically documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) is designed for converting \"bag-of-words\" representations, typically used when classifying or clustering documents. In this case, the training set consists of a list of documents, where each document is represented as a single string.\n",
    "\n",
    "Again, we call `fit_transform` to learn the mapping and then carry out the conversion. We then print the resulting matrix and the learned vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X = ['example text',\n",
    "     'another text']\n",
    "\n",
    "vec = CountVectorizer()\n",
    "Xe = vec.fit_transform(X)\n",
    "print(Xe.toarray())\n",
    "\n",
    "print(vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CountVectorizer` has its own built-in *preprocessor* (text cleaner) and *tokenizer* (word splitter). In some cases, we deal with documents that have already been preprocessed and split into separate words, or we want to carry out those steps separately for some reason. In those cases, we need to disable the built-in preprocessor and tokenizer.\n",
    "\n",
    "In the example below, we do this by providing \"dummy functions\" (the `lambda x: x` part) for the `preprocessor` and `tokenizer` arguments of the `CountVectorizer`'s constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [['example', 'text'],\n",
    "     ['another', 'text']]\n",
    "\n",
    "vec = CountVectorizer(preprocessor = lambda x: x, tokenizer = lambda x: x, token_pattern=None)\n",
    "Xe = vec.fit_transform(X)\n",
    "print(Xe.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also the [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), which is like a `CountVectorizer` except that it includes a method to give a lower weight to words that occur in many documents (IDF, *inverse document frequency*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document classification example\n",
    "\n",
    "To exemplify the use of vectorizers for documents, we take a look at a small collection of bug reports from the [Eclipse](https://www.eclipse.org/) project. \n",
    "\n",
    "This is a simple tab-separated format, where the first column corresponds to the name of the component where the bug occurred, and the second column is the text of the bug report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eclipse_bug_data = pd.read_csv('eclipse_bugs.tsv', sep='\\t', header=None, names=['component', 'bugreport'])\n",
    "\n",
    "eclipse_bug_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use scikit-learn's vectorizers to convert the documents into matrices. We try a `CountVectorizer` as well as a `TfidfVectorizer`; as mentioned previously, a `TfidfVectorizer` is similar to a `CountVectorizer` in that both will compute word frequencies in documents, but the `TfidfVectorizer` also downweights words that occur in many documents. The intuition is that words that appear \"everywhere\" (such as \"and\", \"in\", punctuation) are less informative for predictive tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "Y = eclipse_bug_data.component\n",
    "\n",
    "vectorizer1 = CountVectorizer()\n",
    "X_v1 = vectorizer1.fit_transform(eclipse_bug_data.bugreport)\n",
    "\n",
    "vectorizer2 = TfidfVectorizer()\n",
    "X_v2 = vectorizer2.fit_transform(eclipse_bug_data.bugreport)\n",
    "\n",
    "X_v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_v2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, the result is a matrix with 1000 rows and 13895 columns. What does this tell us about the dataset?\n",
    "\n",
    "We can now use this dataset with any machine learning algorithm in scikit-learn. This time, we try the [perceptron](https://en.wikipedia.org/wiki/Perceptron), a simple mistake-driven algorithm for training linear classifiers. (We will see more of this algorithm in the next lecture.)\n",
    "\n",
    "We get a cross-validation accuracy of about 0.81 with the `CountVectorizer` and 0.82 with the `TfidfVectorizer`. With such a small dataset, this difference is too small to draw any firm conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(Perceptron(), X_v1, Y, cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(Perceptron(), X_v2, Y, cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"hashing trick\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `HashingVectorizer` is a vectorizer similar to `CountVectorizer`, but which does not need to keep a vocabulary. As discussed in the lecture, we sometimes want to avoid building the vocabulary, for instance because we cannot access the whole training set at a time, or because we cannot store the whole training set in memory.\n",
    "\n",
    "The drawbacks of the `HashingVectorizer` is that we cannot inspect the vocabulary, for instance if we'd like to look at the useful features, and that there is a risk of different features \"colliding\" in the converted numerical vectors.\n",
    "\n",
    "Otherwise, we can use the `HashingVectorizer` similarly to a `CountVectorizer`, but note that there is no vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "X = ['example text',\n",
    "     'another text']\n",
    "\n",
    "vec = HashingVectorizer()\n",
    "Xe = vec.fit_transform(X)\n",
    "print(Xe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your information, the `HashingVectorizer` internally uses [*hash functions*](https://en.wikipedia.org/wiki/Hash_function) to map strings to vector-space dimensions. Here is an example showing how you can compute Python's built-in hash function for a couple of strings. (This isn't something you need to care of when using a `HashingVectorizer`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\".__hash__())\n",
    "\n",
    "print(\"hello2\".__hash__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
